{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "971e8f5d",
   "metadata": {},
   "source": [
    "**Exercise 3.1** Devise three example tasks of your own that fit into the MDP framework,\n",
    "identifying for each its states, actions, and rewards. Make the three examples as different\n",
    "from each other as possible. The framework is abstract and flexible and can be applied in\n",
    "many different ways. Stretch its limits in some way in at least one of your examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72e620c",
   "metadata": {},
   "source": [
    ">Walking robot: \n",
    "> - states: angle and position of the legs.\n",
    "> - actions: each move of the legs\n",
    "> - rewards: +1 for each second it stands, -5 each time it falls\n",
    "\n",
    "> Automatic hoven:\n",
    "> - states: The pizza is raw, the pizza is done, the pizza is burned\n",
    "> - actions: more heat, less heat, wait, switch off\n",
    "> - rewards: -1 the pizza is raw, +1 the pizza is perfect, -3 the pizza is burned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeb4edf",
   "metadata": {},
   "source": [
    "**Exercise 3.2** Is the MDP framework adequate to usefully represent all goal-directed\n",
    "learning tasks? Can you think of any clear exceptions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e47c71",
   "metadata": {},
   "source": [
    ">MDP assumes that the state and reward on $t+1$ depends on the state and action at $t$.\n",
    "So one case that doesn't fit can be the K-armed bandit problem, because it doesn't fit with the states assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5279bd25",
   "metadata": {},
   "source": [
    "**Exercise 3.3** Consider the problem of driving. You could define the actions in terms of\n",
    "the accelerator, steering wheel, and brake, that is, where your body meets the machine.\n",
    "Or you could define them farther out—say, where the rubber meets the road, considering\n",
    "your actions to be tire torques. Or you could define them farther in—say, where your\n",
    "brain meets your body, the actions being muscle twitches to control your limbs. Or you\n",
    "could go to a really high level and say that your actions are your choices of where to drive.\n",
    "What is the right level, the right place to draw the line between agent and environment?\n",
    "On what basis is one location of the line to be preferred over another? Is there any\n",
    "fundamental reason for preferring one location over another, or is it a free choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61cc990",
   "metadata": {},
   "source": [
    "> The environment is everything that cannot be changed arbitartry by the agent.\n",
    "With this defintion in mind, the actions should as close as them can be to influence the environment, otherwise it adds an uneccesary compexity. \n",
    "\n",
    ">So considering me as the agent, the actions would be pushing the brakes, steering the wheel, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93389fad",
   "metadata": {},
   "source": [
    "Exercise 3.4 Give a table analogous to that in Example 3.3, but for $p(s', r|s, a)$. It should have columns for $s$, $a$, $s'$, $r$, and $p(s', r|s, a)$, and a row for every 4-tuple for which\n",
    "$p(s', r|s, a) > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f4f5d5",
   "metadata": {},
   "source": [
    "> Example 3.3 Recycling Robot\n",
    "\n",
    "![alt text](./resources/ex_3_3.png \"Cleaning robot\")\n",
    "\n",
    "| s    | a        | s'   || p(s'\\|s,a)   | r(s,a,s')   |\n",
    "|------|----------|------||--------------|-------------|\n",
    "| high | search   | high || $\\alpha$     | $r_{search}$|\n",
    "| high | search   | low  || $(1-\\alpha)$ | $r_{search}$|\n",
    "| low  | search   | high || $(1-\\beta)$  | -3          |\n",
    "| low  | search   | low  || $\\beta$      | $r_{search}$|\n",
    "| high | wait     | high || 1            | $r_{wait}$  |\n",
    "| high | wait     | low  || 0            | -           |\n",
    "| low  | wait     | high || 0            | -           |\n",
    "| low  | wait     | low  || 1            | $r_{wait}$  |\n",
    "| low  | recharge | high || 1            | 0           |\n",
    "| low  | recharge | low  || 0            | -           |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ">| s    | a        | s'   |      r       |p(s',r\\|s,a)|\n",
    ">|------|----------|------|--------------|-------------|\n",
    ">| high | search   | high | $r_{search}$ |  $\\alpha$   |\n",
    ">| high | search   | low  | $r_{search}$ | $(1-\\alpha)$|\n",
    ">| low  | search   | high | $r_{search}$ |      -      |\n",
    ">| low  | search   | low  | $r_{search}$ |  $\\beta$    |\n",
    ">| high | wait     | high | $r_{search}$ |      -      |\n",
    ">| high | wait     | low  | $r_{search}$ |      -      |\n",
    ">| low  | wait     | high | $r_{search}$ |      -      |\n",
    ">| low  | wait     | low  | $r_{search}$ |      -      |\n",
    ">| low  | recharge | high | $r_{search}$ |      -      |\n",
    ">| low  | recharge | low  | $r_{search}$ |      -      |\n",
    ">| high | search   | high |  $r_{wait}$  |      -      |\n",
    ">| high | search   | low  |  $r_{wait}$  |      -      |\n",
    ">| low  | search   | high |  $r_{wait}$  |      -      |\n",
    ">| low  | search   | low  |  $r_{wait}$  |      -      |\n",
    ">| high | wait     | high |  $r_{wait}$  |      1      |\n",
    ">| high | wait     | low  |  $r_{wait}$  |      -      |\n",
    ">| low  | wait     | high |  $r_{wait}$  |      -      |\n",
    ">| low  | wait     | low  |  $r_{wait}$  |      1      |\n",
    ">| low  | recharge | high |  $r_{wait}$  |      -      |\n",
    ">| low  | recharge | low  |  $r_{wait}$  |      -      |\n",
    ">| high | search   | high |     $-3$     |      -      |\n",
    ">| high | search   | low  |     $-3$     |      -      |\n",
    ">| low  | search   | high |     $-3$     | $(1-\\beta)$ |\n",
    ">| low  | search   | low  |     $-3$     |      -      |\n",
    ">| high | wait     | high |     $-3$     |      -      |\n",
    ">| high | wait     | low  |     $-3$     |      -      |\n",
    ">| low  | wait     | high |     $-3$     |      -      |\n",
    ">| low  | wait     | low  |     $-3$     |      -      |\n",
    ">| low  | recharge | high |     $-3$     |      -      |\n",
    ">| low  | recharge | low  |     $-3$     |      -      |\n",
    ">| high | search   | high |     $0$      |      -      |\n",
    ">| high | search   | low  |     $0$      |      -      |\n",
    ">| low  | search   | high |     $0$      |      -      |\n",
    ">| low  | search   | low  |     $0$      |      -      |\n",
    ">| high | wait     | high |     $0$      |      -      |\n",
    ">| high | wait     | low  |     $0$      |      -      |\n",
    ">| low  | wait     | high |     $0$      |      -      |\n",
    ">| low  | wait     | low  |     $0$      |      -      |\n",
    ">| low  | recharge | high |     $0$      |      1      |\n",
    ">| low  | recharge | low  |     $0$      |      -      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411b7631",
   "metadata": {},
   "source": [
    "**Exercise 3.5** The equations in Section 3.1 are for the continuing case and need to be\n",
    "modified (very slightly) to apply to episodic tasks. Show that you know the modifications\n",
    "needed by giving the modified version of (3.3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b175bf6",
   "metadata": {},
   "source": [
    "![alt text](./resources/eq_3_3.png \"Equation 3.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84998937",
   "metadata": {},
   "source": [
    ">$$ \\sum_{s' \\in \\mathbb{S}} \\sum_{r \\in \\mathbb{R}} p(s',r|s,a) = 1, \\quad \\forall  s \\in (\\mathbb{S} - s_T), a \\in \\mathbb{A}(s) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac56cbfb",
   "metadata": {},
   "source": [
    "**Exercise 3.6** Suppose you treated pole-balancing as an episodic task but also used\n",
    "discounting, with all rewards zero except for -1 upon failure. What then would the\n",
    "return be at each time? How does this return differ from that in the discounted, continuing\n",
    "formulation of this task?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081868d2",
   "metadata": {},
   "source": [
    ">The equation would be:\n",
    ">$$G_t = \\sum_{k=0}^T \\gamma^{k} R_{1+k+t}$$\n",
    ">Now, knowing that all rewards would be 0 except the last one that would be -1: \n",
    ">$$G_0 = -1 \\gamma^T$$\n",
    ">$$G_T = -\\gamma$$\n",
    ">that for large T, this would be close to 0 since $0 < \\gamma < 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78b03c0",
   "metadata": {},
   "source": [
    "**Exercise 3.7** Imagine that you are designing a robot to run a maze. You decide to give it a\n",
    "reward of +1 for escaping from the maze and a reward of zero at all other times. The task\n",
    "seems to break down naturally into episodes—the successive runs through the maze—so\n",
    "you decide to treat it as an episodic task, where the goal is to maximize expected total\n",
    "reward (3.7). After running the learning agent for a while, you find that it is showing\n",
    "no improvement in escaping from the maze. What is going wrong? Have you effectively\n",
    "communicated to the agent what you want it to achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6294ea6",
   "metadata": {},
   "source": [
    "> What's happening is that the agent will receive a reward of 1 no matter what, because an episode will end only once the agent has find the output.\n",
    ">\n",
    ">Remember that, for episodic tasks $ G_t = \\sum_t^T R_t$, so in this case $G_T = 0 + \\dots + 0 + 1 = 1 $ no matter how long it takes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cea287",
   "metadata": {},
   "source": [
    "**Exercise 3.8** Suppose $\\gamma = 0.5$ and the following sequence of rewards is received $R_1 = 1$,\n",
    "$R_2 = 2$, $R_3 = 6$, $R_4 = 3$, and $R_5 = 2$, with $T = 5$. What are $G_0$, $G_1$, ..., $G_5$? \n",
    "Hint:\n",
    "Work backwards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b689f0",
   "metadata": {},
   "source": [
    "$$G_t=\\sum_{k=0}^\\infty \\gamma^kR_{1+k+t}$$\n",
    "\n",
    "$R_{1\\dots5} = \\{1,2,6,3,2\\}$\n",
    ">$$G_0 = \\gamma^0 R_1 + \\gamma^1 R_2 + \\gamma^3 R_6 + \\gamma^4 R_3 + \\gamma^5 R_2 = 1·1 + 0.5 · 2 + 0.25·6 + 0.125 · 3 + 0.0625 · 2  = 4$$\n",
    ">$$G_1 = \\gamma^0 R_2 + \\gamma^1 R_3 + \\gamma^4 R_4 + \\gamma^5 R_5 = 6$$\n",
    "$$G_2 = \\gamma^0 R_3 + \\gamma^1 R_4 + \\gamma^3 R_5 = 8$$\n",
    "$$G_3 = \\gamma^0 R_4 + \\gamma^1 R_5 = 4$$\n",
    "$$G_4 = \\gamma^0 R_5 = 2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17870bf4",
   "metadata": {},
   "source": [
    "**Exercise 3.9** Suppose  $\\gamma = 0.9$ and the reward sequence is $R_1 = 2$ followed by an infinite\n",
    "sequence of 7s. What are $G_1$ and $G_0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef83e3c7",
   "metadata": {},
   "source": [
    "$$S_n = \\sum_{k=0}^N r^kc = c + cr + cr^2 + \\dots + cr^N\\newline\n",
    "rS_n = \\sum_{k=0}^N r^{k+1}c = cr + cr^2 + \\dots + cr^{N+1}\\newline\n",
    "S_n - rS_n = c - cr^{N+1} \\newline \n",
    "S_n = \\sum_{k=0}^N r^kc = \\frac{c - cr^{N+1}}{1-r}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9f2f6c",
   "metadata": {},
   "source": [
    ">$$G_t=\\sum_{k=0}^\\infty \\gamma^kR_{1+k+t}$$\n",
    ">\n",
    ">$$G_1 = \\sum_{k=0}^\\infty\\gamma^kR_{2+k} = \\frac{7(1-0.9^\\infty)}{1-0.9} = \\frac{7}{0.1} = 70 $$\n",
    "$$G_0 = R_1 + \\gamma G_1 = 2 + 0.9·70 = 65$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb312b0b",
   "metadata": {},
   "source": [
    "**Exercise 3.10** Prove the second equality in (3.10)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa10a61e",
   "metadata": {},
   "source": [
    "$$G_t=\\sum_{k=0}^\\infty \\gamma^k$$\n",
    ">$$\\gamma G_t=\\sum_{k=0}^\\infty \\gamma^{k+1} \\newline\n",
    "G_t - \\gamma G_t = 1 - \\gamma^{\\infty + 1} \\newline\n",
    "G_t = \\frac{1}{1-\\gamma}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f944545d",
   "metadata": {},
   "source": [
    "**Exercise 3.11** If the current state is $S_t$, and actions are selected according to a stochastic\n",
    "policy $\\pi$, then what is the expectation of $R_{t+1}$ in terms of $\\pi$ and the four-argument\n",
    "function $p$ (3.2)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bfd022",
   "metadata": {},
   "source": [
    "![alt text](./resources/eq_3_2.png \"Function p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2b2464",
   "metadata": {},
   "source": [
    ">$\\mathbb{E}[R_{t+1}] = \\sum_r rp(r|S_t=s) \\newline$\n",
    ">By marginalization and conditional probability:\n",
    ">$$\\mathbb{E}[R_{t+1}] = \\sum_a\\sum_r rp(r,a|S_t=s) = \\sum_a\\sum_r rp(r|S_t=s, a)P(a|S_t=s) = \\sum_aP(a|S_t=s)\\sum_r rp(r|S_t=s, a)$$\n",
    "Again by marginaliztion:\n",
    ">$$\\mathbb{E}[R_{t+1}] = \\sum_aP(a|S_t=s)\\sum_s'\\sum_r rp(r,s'|S_t=s, a)$$\n",
    ">$$\\mathbb{E}[R_{t+1}] = \\sum_a\\pi(a|S_t=s)\\sum_s'\\sum_r rp(r,s'|S_t=s, a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9213ef08",
   "metadata": {},
   "source": [
    "**Exercise 3.12** Give an equation for $v_\\pi$ in terms of $q_\\pi$ and $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ec749c",
   "metadata": {},
   "source": [
    ">Tip (Marginaization): \n",
    ">$$P(X)=\\sum_yP(X,Y=y)=\\sum_yP(X|Y=y)P(Y=y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046b98f5",
   "metadata": {},
   "source": [
    ">Knowing: $$q_\\pi(s,a) = \\mathbb{E}_\\pi[G_t|S_t=s, A_t=a] $$\n",
    ">and assuming policy $\\pi$ is just equal probability\n",
    ">$$v_\\pi(s) = \\mathbb{E}_\\pi[G_t|S_t=s] \\newline\n",
    "= \\sum_{a \\in \\mathbb{A}}\\mathbb{E}_\\pi[G_t, A_t=a|S_t=s] \\newline\n",
    "= \\sum_{a \\in \\mathbb{A}} P(a|s)\\mathbb{E}_\\pi[G_t|S_t=s, A_t=a]  \\newline\n",
    "= \\sum_{a \\in \\mathbb{A}} \\pi(a|s)q_\\pi(s,a)  \\newline$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855ab5b8",
   "metadata": {},
   "source": [
    "**Exercise 3.13** Give an equation for ${q_\\pi}$ in terms of ${v_\\pi}$ and the four-argument ${p}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a682196",
   "metadata": {},
   "source": [
    "$$q_\\pi(s,a) = \\mathbb{E}_\\pi[G_t|S_t=s, A_t=a]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db9636b",
   "metadata": {},
   "source": [
    "![image.png](./resources/backup_diagram_v.png \"Example backup-diagram v(s)\")\n",
    "![alt text](./resources/eq_3_14.png \"Bellman equation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9140d0",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "**Exercise 3.14** The Bellman equation must hold for each state for the value function\n",
    "$v_\\pi$ shown in Figure 3.2 (right). Show numerically that this equation holds\n",
    "for the center state, valued at +0.7, with respect to its four neighboring states, valued at\n",
    "+2.3, +0.4, 0.4, and +0.7. (These numbers are accurate only to one decimal place.)\n",
    "\n",
    "![alt text](./resources/figure_3_2.png \"GridWorld example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1998e51",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    ">$A = \\{\\rightarrow, \\leftarrow, \\uparrow, \\downarrow\\}\\\\\n",
    "\\pi(a|s) = P(a|s) = 0.25 \\: \\forall \\: a, s \\\\\n",
    "P(s'=s+\\rightarrow|a=\\rightarrow) = 1 \\\\\n",
    "P(s'=s+\\leftarrow|a=\\leftarrow) = 1 \\\\\n",
    "P(s'=s+\\uparrow|a=\\uparrow) = 1 \\\\\n",
    "P(s'=s+\\downarrow|a=\\downarrow) = 1 \\newline\n",
    "v(s) = 0.7 \\\\\n",
    "v(s') = \\{2.3, 0.7, 0.4, -0.4\\} \\\\\n",
    "r=0 \\\\\n",
    "\\gamma=0.9$\n",
    ">\n",
    "><br>**Bellman Equation**: $$\\sum_a\\pi(a|s)\\sum_{s', r}p(s', r|s, a)\\:[r + \\gamma v_{\\pi}(s')]$$\n",
    ">\n",
    ">$$ v(s) = 0.25 [0 · [0 + 0.9 · 2.3] \n",
    "        + 0 · [0 + 0.9 · 0.7]\n",
    "        + 1 · [0 + 0.9 · 0.4]\n",
    "        + 0 · [0 + 0.9 · -0.4]]\\\\\n",
    "        +0.25 [0 · [0 + 0.9 · 2.3] \n",
    "        + 1 · [0 + 0.9 · 0.7]\n",
    "        + 0 · [0 + 0.9 · 0.4]\n",
    "        + 0 · [0 + 0.9 · -0.4]]\\\\\n",
    "        +0.25 [1 · [0 + 0.9 · 2.3] \n",
    "        + 0 · [0 + 0.9 · 0.7]\n",
    "        + 0 · [0 + 0.9 · 0.4]\n",
    "        + 0 · [0 + 0.9 · -0.4]]\\\\\n",
    "        +0.25 [0 · [0 + 0.9 · 2.3] \n",
    "        + 0 · [0 + 0.9 · 0.7]\n",
    "        + 0 · [0 + 0.9 · 0.4]\n",
    "        + 1 · [0 + 0.9 · -0.4]] \\\\= 0.675 \\approx 0.7$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6c898a",
   "metadata": {},
   "source": [
    "![alt text](./resources/eq_3_8.png \"Discounted return\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207417e1",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "**Exercise 3.15** In the gridworld example, rewards are positive for goals, negative for\n",
    "running into the edge of the world, and zero the rest of the time. Are the signs of these\n",
    "rewards important, or only the intervals between them? Prove, using (3.8), that adding a\n",
    "constant $c$ to all the rewards adds a constant, $v_c$, to the values of all states, and thus\n",
    "does not affect the relative values of any states under any policies. What is $v_c$ in terms\n",
    "of $c$ and $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac7f436",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "Tip:\n",
    "$$S=\\sum_{k=0}^K r^k = 1 +r +r^2 + r^3 + \\dots + r^K \\quad \\forall \\; r < 1$$\n",
    "Multipliying by $r$:\n",
    "$rS = r + r^2 + r^3 + r^4 \\dots + r^{K+1} \\newline\n",
    "S - rS = (1-r)S = 1 - r^{K+1} \\newline\n",
    "$\n",
    "$$\\sum_{k=0}^K r^k = \\frac{1 - r^{K+1}}{(1-r)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d444821c",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "$G_t = \\sum^\\infty_{k=0} \\gamma^kR_{t+k+1} \\\\$\n",
    "Now adding a constant $C$: $\\hat{G_t} = \\sum^\\infty_{k=0} \\gamma^k(R_{t+k+1} + C) \\newline\n",
    "= \\sum_{k=1}^\\infty\\gamma^kR_{t+k+1}+\\sum_{k=1}^\\infty\\gamma^kC \\newline\n",
    "= \\sum_{k=1}^\\infty\\gamma^kR_{t+k+1}+C\\frac{1 - \\gamma^\\infty}{1-\\gamma} =\\frac{C}{1-\\gamma} + \\sum_{k=1}^\\infty\\gamma^kR_{t+k+1}$ \n",
    "\n",
    "$$v_c = \\frac{C}{1-\\gamma}$$\n",
    "\n",
    "$v_\\pi(s) = \\mathbb{E}[G_t|S_t=s] \\newline\n",
    "= \\mathbb{E}[v_c + R_{t+1} + G_{t+1}|S_t=s] \\newline\n",
    "= v_c +\\sum_a \\pi(a|s) \\sum_{s', r}(s', r|s,a)[r + \\gamma v_\\pi(s')] \\quad \\forall \\: s \\: \\in \\: S$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eae9d4",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "**Exercise 3.16** Now consider adding a constant $c$ to all the rewards in an episodic task,\n",
    "such as maze running. Would this have any effect, or would it leave the task unchanged\n",
    "as in the continuing task above? Why or why not? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3060f4",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "$G_t = \\sum_{k=0}^K(R_{t+k+1} + C) = KC + \\sum_{k=0}^KR_{t+k+1}\\newline\n",
    "G_t = KC + R_{t+1} + G_{t+1}\\newline$\n",
    "$$v_\\pi(s) = \\mathbb{E}[G_t|S_t=s] \\newline\n",
    "= \\mathbb{E}[KC + R_{t+1} + G_{t+1}|S_t=s] \\newline\n",
    "= KC +\\sum_a \\pi(a|s) \\sum_{s', r}(s', r|s,a)[r + v_\\pi(s')] \\quad \\forall \\: s \\: \\in \\: S$$\n",
    "\n",
    "As in the continuing task above, this would have no effect since $v_\\pi(s)$ is the same, only adding a constant value $KC$ which would have no effect when choosing among all the available actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7caa568",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "**Exercise 3.17** What is the Bellman equation for action values, that\n",
    "is, for $q$? It must give the action value $q(s, a)$ in terms of the action\n",
    "values, $q(s_0, a_0)$, of possible successors to the state–action pair $(s, a)$.\n",
    "Hint: The backup diagram to the right corresponds to this equation.\n",
    "Show the sequence of equations analogous to (3.14), but for action\n",
    "values. ![image.png](./resources/backup_diagram_q.png \"Example backup-diagram q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b16c36",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "$$q_\\pi(s,a) = \\mathbb{E}[G_t|S_t=s, A_t=a] \\newline\n",
    "= \\mathbb{E}[R_{t+1} + G_{t+1}|S_t=s, A_t=a] \\newline\n",
    "= \\sum_{s', r}P(s', r | a, s)[r + \\mathbb{E}_\\pi [G_{t+1}|S_t=s]] \\newline\n",
    "= \\sum_{s', r}P(s', r | a, s)[r + \\sum_{s'}\\pi(a|s')\\mathbb{E}_\\pi [G_{t+1}|S_t=s, A_t=a]] \\newline \n",
    "= \\sum_{s', r}P(s', r | a, s)[r + \\sum_{s'}\\pi(a'|s')q_\\pi(s',a')] \\newline$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17b541a",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "**Exercise 3.18** The value of a state depends on the values of the actions possible in that\n",
    "state and on how likely each action is to be taken under the current policy. We can\n",
    "think of this in terms of a small backup diagram rooted at the state and considering each\n",
    "possible action:\n",
    "\n",
    "![image.png](./resources/ex_3_18.png \"backup diagram ex:18\")\n",
    "\n",
    "Give the equation corresponding to this intuition and diagram for the value at the root\n",
    "node, $v(s)$, in terms of the value at the expected leaf node, $q(s, a)$, given $S_t = s$. This\n",
    "equation should include an expectation conditioned on following the policy, $\\pi$. Then give\n",
    "a second equation in which the expected value is written out explicitly in terms of $\\pi(a|s)$\n",
    "such that no expected value notation appears in the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d72f043",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "$$v_\\pi(s) = \\mathbb{E}[G_t|S_t=s] \\newline\n",
    "= \\sum_{a \\in A}P(a|S_t=s)\\mathbb{E}[G_t|S_t=s, A_t=a] \\newline\n",
    "= \\sum_{a \\in A}P(a|S_t=s)q_\\pi(s,a) \\newline\n",
    "= \\sum_{a \\in A} \\pi(a|s)q_\\pi(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601430cd",
   "metadata": {},
   "source": [
    "**Exercise 3.19** The value of an action, $q(s, a)$, depends on the expected next reward and\n",
    "the expected sum of the remaining rewards. Again we can think of this in terms of a\n",
    "small backup diagram, this one rooted at an action (state–action pair) and branching to\n",
    "the possible next states:\n",
    "\n",
    "![image.png](./resources/ex_3_19.png \"backup diagram ex:19\")\n",
    "\n",
    "Give the equation corresponding to this intuition and diagram for the action value,\n",
    "$q(s, a)$, in terms of the expected next reward, $R_{t+1}$, and the expected next state value,\n",
    "$v(S_{t+1})$, given that $S_t =s$ and $A_t =a$. This equation should include an expectation but\n",
    "not one conditioned on following the policy. Then give a second equation, writing out the\n",
    "expected value explicitly in terms of $p(s0, r|s, a)$ defined by (3.2), such that no expected\n",
    "value notation appears in the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b3e767",
   "metadata": {},
   "source": [
    "$$q_\\pi(s,a) = \\mathbb{E}_\\pi[G_t|S_t=s, A_t=a] = \\sum_{s',r}P(s, r'|s,a)[r + v_\\pi(s')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a2ebb8",
   "metadata": {},
   "source": [
    "**Exercise 3.20** Draw or describe the optimal state-value function for the golf example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731cdb29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6799dc7c",
   "metadata": {},
   "source": [
    "**Exercise 3.21** Draw or describe the contours of the optimal action-value function for putting, $q_*(s, putter)$, for the golf example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3597f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54a15b8a",
   "metadata": {},
   "source": [
    "**Exercise 3.22** Consider the continuing MDP shown to the right. The only decision to be made is that in the top state,\n",
    "where two actions are available, $left$ and $right$. The numbers show the rewards that are received deterministically after\n",
    "each action. There are exactly two deterministic policies, $\\pi_{left}$ and $\\pi_{right}$. What policy is optimal if  $\\gamma = 0$? If  $\\gamma = 0.9$?\n",
    "If  $\\gamma = 0.5$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b566fdb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "110fb7eb",
   "metadata": {},
   "source": [
    "**Exercise 3.23** Give the Bellman equation for $q_*$ for the recycling robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38e6eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a066ee5",
   "metadata": {},
   "source": [
    "**Exercise 3.24** Figure 3.5 gives the optimal value of the best state of the gridworld as 24.4, to one decimal place. Use your knowledge of the optimal policy and (3.8) to express\n",
    "this value symbolically, and then to compute it to three decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d4b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7311a857",
   "metadata": {},
   "source": [
    "**Exercise 3.25** Give an equation for $v_*$ in terms of $q_*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9429ff2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b2e240a",
   "metadata": {},
   "source": [
    "**Exercise 3.26** Give an equation for $q_*$ in terms of $v_*$ and the four-argument $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91be1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "508dcd41",
   "metadata": {},
   "source": [
    "**Exercise 3.27** Give an equation for $\\pi_*$ in terms of $q_*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60163da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af521ad6",
   "metadata": {},
   "source": [
    "**Exercise 3.28** Give an equation for $\\pi_*$ in terms of $v_*$ and the four-argument $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc099f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "820d79f1",
   "metadata": {},
   "source": [
    "**Exercise 3.29** Rewrite the four Bellman equations for the four value functions ($v_\\pi$, $v_*$, $q_\\pi$, and $q_*$) in terms of the three argument function p (3.4) and the two-argument function $r$\n",
    "(3.5)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
